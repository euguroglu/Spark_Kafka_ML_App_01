from pyspark.sql import SparkSession
import json
from pyspark.sql.types import *
from pyspark.sql.functions import *
import requests

#Create spark session
spark = SparkSession \
    .builder \
    .appName('RealtimeKafkaML') \
    .master("local[3]") \
    .config("spark.streaming.stopGracefullyOnShutdown", "true") \
    .getOrCreate()

#Read message from kafka KafkaProducer
#kafka_producer.py generates message
kafka_df = spark \
    .readStream \
    .format('kafka') \
    .option('kafka.bootstrap.servers', "localhost:9092") \
    .option("startingOffsets", "earliest") \
    .option('subscribe', 'test') \
    .load()

#Schema is created to define scheme for message generated by kafka_producer.py
#When you look at kafka_producer key value is "data"
schema = StructType([StructField('data', StringType())])
#Schema_output is create to define schema when we get response from machine learning model
schema_output = StructType([StructField('neg', StringType()),\
                            StructField('pos', StringType()),\
                            StructField('neu', StringType()),\
                            StructField('compound', StringType())])
kafka_df.printSchema()
#Function to send post request to flask application
#Flask application contains machine learning model
#Model makes sentiment analysis on the given text
def apply_sentiment_analysis(data):

    result = requests.post('http://localhost:5000/predict', json=json.loads(data))
    return json.dumps(result.json())

#User defined function created to send read value to flask
vader_udf = udf(lambda data: apply_sentiment_analysis(data), StringType())

value_df = kafka_df.select(from_json(col("value").cast("string"),schema).alias("sentence"),\
                           from_json(vader_udf(col("value").cast("string")),schema_output).alias("response"))
value_df.printSchema()

#We select sentence and response values
explode_df = value_df.select("sentence.data", "response.*")

#Write stream to console
console_query = explode_df \
    .writeStream \
    .trigger(processingTime="1 minute") \
    .format("console") \
    .outputMode("append") \
    .option("checkpointLocation", "chk-point-dir") \
    .start()

console_query.awaitTermination()
############Test kafka producer if spark reads kafka message correctly###########
#You can use below code first before construct machine learning pipeline
# value_df = kafka_df.select(from_json(col("value").cast("string"),schema).alias("value"))
#
# value_df.printSchema()
#
# explode_df = value_df.selectExpr("value.data")
#
# explode_df.printSchema()
#
# explode_df = explode_df.select('*')
#
# console_query = explode_df \
#     .writeStream \
#     .trigger(once=True) \
#     .format("console") \
#     .outputMode("append") \
#     .option("checkpointLocation", "chk-point-dir") \
#     .start()
#
# console_query.awaitTermination()
